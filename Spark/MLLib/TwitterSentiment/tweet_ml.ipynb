{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AbhishekRawat:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.textFile(\"C:/Users/Abhishek/Downloads/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ItemID,Sentiment,SentimentText',\n",
       " '1,0,                     is so sad for my APL friend.............',\n",
       " '2,0,                   I missed the New Moon trailer...',\n",
       " '3,1,              omg its already 7:30 :O',\n",
       " \"4,0,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ItemID,Sentiment,SentimentText'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header=rdd.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the header from the RDD\n",
    "\n",
    "rdd=rdd.filter(lambda line:line!=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,0,                     is so sad for my APL friend.............',\n",
       " '2,0,                   I missed the New Moon trailer...',\n",
       " '3,1,              omg its already 7:30 :O',\n",
       " \"4,0,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\",\n",
       " '5,0,         i think mi bf is cheating on me!!!       T_T']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=rdd.map(lambda line:line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '0', '                     is so sad for my APL friend.............'],\n",
       " ['2', '0', '                   I missed the New Moon trailer...']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing S.No. from the RDD\n",
    "\n",
    "rdd=rdd.map(lambda line:line[1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '                     is so sad for my APL friend.............'],\n",
       " ['0', '                   I missed the New Moon trailer...']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rg1=re.compile(r'\\d+') #For eliminating digits\n",
    "rg2=re.compile(r'[\\\"\\',-.:;&!#$%^*<>=?~_/]') #For eliminating all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "#stem = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#Just using Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transf(tweet):\n",
    "    tweet=tweet.lower()\n",
    "    tweet=rg1.sub(\"\",tweet) \n",
    "    tweet=rg2.sub(\"\",tweet)\n",
    "    tweet=tweet.strip().split(\" \") #Separating the words\n",
    "    \n",
    "    for word in tweet:\n",
    "        word=lm.lemmatize(word,\"v\")\n",
    "        #word=stem.stem(word) \n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df=rdd.map(lambda line:Row(label=line[0],\n",
    "                          words=transf(line[1]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to convert to DF\n",
    "\n",
    "#rdd_new=rdd.map(lambda line:{\"label\":line[0],\"words\":transf(line[1])})\n",
    "#df=rdd_new.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|               words|\n",
      "+-----+--------------------+\n",
      "|    0|[is, so, sad, for...|\n",
      "|    0|[i, missed, the, ...|\n",
      "|    1|[omg, its, alread...|\n",
      "|    0|[omgaga, im, sooo...|\n",
      "|    0|[i, think, mi, bf...|\n",
      "|    0|[or, i, just, wor...|\n",
      "|    1|[juuuuuuuuuuuuuuu...|\n",
      "|    0|[sunny, again, , ...|\n",
      "|    1|[handed, in, my, ...|\n",
      "|    1|[hmmmm, i, wonder...|\n",
      "|    0|[i, must, think, ...|\n",
      "|    1|[thanks, to, all,...|\n",
      "|    0|[this, weekend, h...|\n",
      "|    0|[jb, isnt, showin...|\n",
      "|    0|[ok, thats, it, y...|\n",
      "|    0|[lt, this, is, th...|\n",
      "|    0|[awhhe, man, im, ...|\n",
      "|    1|[feeling, strange...|\n",
      "|    0|[huge, roll, of, ...|\n",
      "|    0|[i, just, cut, my...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filteredWords\")\n",
    "df=remover.transform(df)\n",
    "\n",
    "#Removed all the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only using TF as IDF lead to lower accuracy. This may be due to the fact that \n",
    "#IDF tries to find the tags unique to each tweet. Whereas, in case of sentiments,\n",
    "#many similar words might lead to a positive sentiment in the tweets.\n",
    "#Ex. the word \"happy\" may denote a positive tweet even if it appears in many tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filteredWords\", outputCol=\"features\")\n",
    "rescaledData = hashingTF.transform(df)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "rescaledData=rescaledData.withColumn(\"label\",rescaledData[\"label\"].cast(IntegerType()))\n",
    "\n",
    "train,test=rescaledData.randomSplit([0.7,0.3]) #30% kept as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now trying out various classification techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy = 0.664379\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "predictions=lrModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Test set Accuracy = %g\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy = 0.702819\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(train)\n",
    "\n",
    "predictions=lsvcModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Test set Accuracy = %g\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.7218390034824538\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "#Creating the trainer and set its parameters\n",
    "nb = NaiveBayes(modelType=\"multinomial\")\n",
    "\n",
    "#Training the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "#Computing accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes led to the best accuracy of on the test set. Thus we will save that model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"TwitterSentimentNB.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
